{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SathishMalaVish/Data-Driven-Stock-Analysis/blob/main/Final_Project_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit\n",
        "!pip install mysql-connector-python\n",
        "!pip install pyngrok\n",
        "!npm install localtunnel\n",
        "!pip install ngrok\n",
        "!pip install mysql\n",
        "!pip install requests pandas pymysql streamlit"
      ],
      "metadata": {
        "id": "X_wpX3n-9qdP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "2d5094bf-3e26-413e-fc87-b029606bdcae"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.42.2-py2.py3-none-any.whl.metadata (8.9 kB)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.1)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.1.8)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.26.4)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.1.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.25.6)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (17.0.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (13.9.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (9.0.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.12.2)\n",
            "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m642.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.5)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.27.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.1.31)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich<14,>=10.14.0->streamlit) (2.18.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.22.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Downloading streamlit-1.42.2-py2.py3-none-any.whl (9.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: watchdog, pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 streamlit-1.42.2 watchdog-6.0.0\n",
            "Collecting mysql-connector-python\n",
            "  Downloading mysql_connector_python-9.2.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.0 kB)\n",
            "Downloading mysql_connector_python-9.2.0-cp311-cp311-manylinux_2_28_x86_64.whl (34.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.0/34.0 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: mysql-connector-python\n",
            "Successfully installed mysql-connector-python-9.2.0\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.3-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Downloading pyngrok-7.2.3-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.3\n",
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K\n",
            "added 22 packages in 2s\n",
            "\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K\n",
            "\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K3 packages are looking for funding\n",
            "\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K  run `npm fund` for details\n",
            "\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0KCollecting ngrok\n",
            "  Downloading ngrok-1.4.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
            "Downloading ngrok-1.4.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ngrok\n",
            "Successfully installed ngrok-1.4.0\n",
            "Collecting mysql\n",
            "  Downloading mysql-0.0.3-py3-none-any.whl.metadata (746 bytes)\n",
            "Collecting mysqlclient (from mysql)\n",
            "  Downloading mysqlclient-2.2.7.tar.gz (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.4/91.4 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Downloading mysql-0.0.3-py3-none-any.whl (1.2 kB)\n",
            "Building wheels for collected packages: mysqlclient\n",
            "  Building wheel for mysqlclient (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mysqlclient: filename=mysqlclient-2.2.7-cp311-cp311-linux_x86_64.whl size=127529 sha256=65273d7dc0db180935861a3561b1d6a83d32c2a670006058687d2f325ef2d8a3\n",
            "  Stored in directory: /root/.cache/pip/wheels/cd/5f/fb/dbe9e6e7e1e24f9e73164c77b5fdc19bd36240425835c0e635\n",
            "Successfully built mysqlclient\n",
            "Installing collected packages: mysqlclient, mysql\n",
            "Successfully installed mysql-0.0.3 mysqlclient-2.2.7\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Collecting pymysql\n",
            "  Downloading PyMySQL-1.1.1-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.11/dist-packages (1.42.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.1)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.1.8)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.1.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.25.6)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (17.0.0)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (13.9.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (9.0.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.12.2)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.5)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.27.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich<14,>=10.14.0->streamlit) (2.18.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.22.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Downloading PyMySQL-1.1.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.0/45.0 kB\u001b[0m \u001b[31m292.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymysql\n",
            "Successfully installed pymysql-1.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting all Yaml file into CSV file, combine and save the CSV files, symbol wise csv file save\n",
        "\n",
        "import os\n",
        "import yaml\n",
        "import csv\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Update paths to Google Drive or Colab directory\n",
        "folder_path = \"/content/drive/My Drive/Data-Driven Stock Analysis\"\n",
        "output_dir = \"/content/drive/My Drive/Assignment_CSV/symbol_CSV\"\n",
        "\n",
        "# Ensure output directory exists\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "def extract_and_transform_data(folder_path, output_dir):\n",
        "    symbol_data = defaultdict(list)\n",
        "\n",
        "    # Debugging: List files\n",
        "    print(\"Scanning folder:\", folder_path)\n",
        "\n",
        "    for root, dirs, files in os.walk(folder_path):\n",
        "        for file in files:\n",
        "            if file.endswith('.yaml'):\n",
        "                file_path = os.path.join(root, file)\n",
        "                print(f\"Processing file: {file_path}\")\n",
        "\n",
        "                try:\n",
        "                    with open(file_path, 'r', encoding='utf-8') as yaml_file:\n",
        "                        yaml_content = yaml.safe_load(yaml_file)\n",
        "\n",
        "                        if yaml_content and isinstance(yaml_content, list):\n",
        "                            for entry in yaml_content:\n",
        "                                if isinstance(entry, dict) and 'Ticker' in entry:\n",
        "                                    ticker = entry['Ticker']\n",
        "                                    flattened_data = {\n",
        "                                        'ticker': ticker,\n",
        "                                        'date': entry.get('date', ''),\n",
        "                                        'close': entry.get('close', ''),\n",
        "                                        'high': entry.get('high', ''),\n",
        "                                        'low': entry.get('low', ''),\n",
        "                                        'open': entry.get('open', ''),\n",
        "                                        'volume': entry.get('volume', ''),\n",
        "                                        'month': entry.get('month', '')\n",
        "                                    }\n",
        "                                    symbol_data[ticker].append(flattened_data)\n",
        "                                else:\n",
        "                                    print(f\"Invalid entry in {file_path}\")\n",
        "                        else:\n",
        "                            print(f\"Invalid YAML format: {file_path}\")\n",
        "\n",
        "                except yaml.YAMLError as e:\n",
        "                    print(f\"Error reading YAML: {file_path}, {e}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Unexpected error: {file_path}, {e}\")\n",
        "\n",
        "    # Write to CSV\n",
        "    for symbol, entries in symbol_data.items():\n",
        "        output_csv = os.path.join(output_dir, f\"{symbol}.csv\")\n",
        "        with open(output_csv, 'w', newline='', encoding='utf-8') as csv_file:\n",
        "            fieldnames = ['ticker', 'date', 'close', 'high', 'low', 'open', 'volume', 'month']\n",
        "            csv_writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
        "            csv_writer.writeheader()\n",
        "\n",
        "            for entry in entries:\n",
        "                if isinstance(entry, dict):\n",
        "                    csv_writer.writerow(entry)\n",
        "                else:\n",
        "                    print(f\"Unexpected data format: {entry}\")\n",
        "\n",
        "        print(f\"CSV created: {output_csv}\")\n",
        "\n",
        "extract_and_transform_data(folder_path, output_dir)\n",
        "\n",
        "save_path = \"/content/drive/My Drive/Assignment_CSV/combined_data.csv\"  # Path to save the combined CSV\n",
        "\n",
        "# Load all CSV files\n",
        "def load_all_csvs_and_save(output_dir, save_path):\n",
        "    data_frames = []\n",
        "    print(\"📂 Scanning CSV files in:\", output_dir)\n",
        "\n",
        "    for file in os.listdir(output_dir):\n",
        "        if file.endswith('.csv'):\n",
        "            file_path = os.path.join(output_dir, file)\n",
        "            print(f\"📄 Loading file: {file_path}\")\n",
        "            try:\n",
        "                df = pd.read_csv(file_path)\n",
        "                df['ticker'] = file.replace('.csv', '')\n",
        "                df['date'] = pd.to_datetime(df['date'], errors='coerce')  # Convert date column to datetime\n",
        "                data_frames.append(df)\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error loading {file_path}: {e}\")\n",
        "\n",
        "    if data_frames:\n",
        "        # Combine all dataframes into one\n",
        "        combined_df = pd.concat(data_frames, ignore_index=True)\n",
        "        combined_df = combined_df.sort_values(by=['ticker', 'date'])\n",
        "\n",
        "        # Save the combined DataFrame to a new CSV file\n",
        "        combined_df.to_csv(save_path, index=False)\n",
        "        print(f\"✅ Combined data saved to: {save_path}\")\n",
        "    else:\n",
        "        print(\"❌ No CSV files found or loaded.\")\n",
        "\n",
        "# Run the function to load, combine, and save the CSV files\n",
        "load_all_csvs_and_save(output_dir, save_path)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "bZEQ0Ka_9qge",
        "outputId": "ea48f32b-e685-4f7a-db9b-36e3b7ff3e06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Scanning folder: /content/drive/My Drive/Data-Driven Stock Analysis\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-11/2024-11-11_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-11/2024-11-21_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-11/2024-11-18_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-11/2024-11-22_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-11/2024-11-04_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-11/2024-11-19_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-11/2024-11-08_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-11/2024-11-06_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-11/2024-11-07_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-11/2024-11-13_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-11/2024-11-14_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-11/2024-11-01_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-11/2024-11-05_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-11/2024-11-12_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-01/2024-01-16_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-01/2024-01-04_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-01/2024-01-19_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-01/2024-01-08_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-01/2024-01-09_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-01/2024-01-12_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-01/2024-01-05_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-01/2024-01-18_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-01/2024-01-17_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-01/2024-01-29_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-01/2024-01-02_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-01/2024-01-10_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-01/2024-01-01_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-01/2024-01-11_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-01/2024-01-15_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-01/2024-01-03_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-01/2024-01-31_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-01/2024-01-23_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-01/2024-01-20_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-01/2024-01-24_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-01/2024-01-30_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-01/2024-01-25_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-10/2024-10-01_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-10/2024-10-07_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-10/2024-10-21_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-10/2024-10-04_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-10/2024-10-29_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-10/2024-10-10_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-10/2024-10-23_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-10/2024-10-24_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-10/2024-10-08_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-10/2024-10-14_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-10/2024-10-28_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-10/2024-10-11_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-10/2024-10-03_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-10/2024-10-31_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-10/2024-10-15_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-10/2024-10-22_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-10/2024-10-16_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-10/2024-10-30_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-10/2024-10-09_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-10/2024-10-18_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-10/2024-10-17_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-10/2024-10-25_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-09/2024-09-02_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-09/2024-09-09_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-09/2024-09-19_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-09/2024-09-04_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-09/2024-09-23_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-09/2024-09-10_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-09/2024-09-12_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-09/2024-09-20_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-09/2024-09-05_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-09/2024-09-06_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-09/2024-09-18_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-09/2024-09-11_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-09/2024-09-03_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-09/2024-09-25_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-09/2024-09-30_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-09/2024-09-24_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-09/2024-09-16_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-09/2024-09-26_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-09/2024-09-27_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-09/2024-09-17_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-09/2024-09-13_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-05/2024-05-10_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-05/2024-05-09_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-05/2024-05-23_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-05/2024-05-17_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-05/2024-05-30_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-05/2024-05-07_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-05/2024-05-08_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-05/2024-05-29_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-05/2024-05-27_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-05/2024-05-13_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-05/2024-05-22_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-05/2024-05-03_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-05/2024-05-24_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-05/2024-05-06_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-05/2024-05-21_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-05/2024-05-18_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-05/2024-05-31_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-05/2024-05-15_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-05/2024-05-02_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-05/2024-05-16_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-05/2024-05-28_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-05/2024-05-14_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-02/2024-02-13_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-02/2024-02-27_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-02/2024-02-20_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-02/2024-02-07_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-02/2024-02-29_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-02/2024-02-16_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-02/2024-02-15_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-02/2024-02-12_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-02/2024-02-23_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-02/2024-02-09_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-02/2024-02-28_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-02/2024-02-19_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-02/2024-02-08_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-02/2024-02-02_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-02/2024-02-14_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-02/2024-02-21_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-02/2024-02-22_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-02/2024-02-01_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-02/2024-02-06_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-02/2024-02-26_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-02/2024-02-05_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-04/2024-04-01_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-04/2024-04-03_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-04/2024-04-04_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-04/2024-04-02_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-04/2024-04-12_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-04/2024-04-16_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-04/2024-04-15_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-04/2024-04-22_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-04/2024-04-18_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-04/2024-04-05_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-04/2024-04-08_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-04/2024-04-10_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-04/2024-04-26_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-04/2024-04-24_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-04/2024-04-25_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-04/2024-04-19_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-04/2024-04-30_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-04/2024-04-23_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-04/2024-04-29_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-04/2024-04-09_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-08/2024-08-05_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-08/2024-08-02_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-08/2024-08-01_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-08/2024-08-14_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-08/2024-08-07_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-08/2024-08-26_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-08/2024-08-30_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-08/2024-08-12_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-08/2024-08-27_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-08/2024-08-09_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-08/2024-08-19_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-08/2024-08-16_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-08/2024-08-28_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-08/2024-08-13_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-08/2024-08-06_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-08/2024-08-20_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-08/2024-08-23_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-08/2024-08-22_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-08/2024-08-29_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-08/2024-08-08_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-08/2024-08-21_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-07/2024-07-22_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-07/2024-07-26_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-07/2024-07-16_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-07/2024-07-10_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-07/2024-07-12_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-07/2024-07-03_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-07/2024-07-29_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-07/2024-07-31_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-07/2024-07-23_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-07/2024-07-18_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-07/2024-07-08_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-07/2024-07-19_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-07/2024-07-25_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-07/2024-07-30_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-07/2024-07-15_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-07/2024-07-24_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-07/2024-07-01_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-07/2024-07-11_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-07/2024-07-02_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-07/2024-07-09_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-07/2024-07-04_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-07/2024-07-05_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-03/2024-03-05_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-03/2024-03-27_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-03/2024-03-28_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-03/2024-03-18_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-03/2024-03-02_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-03/2024-03-19_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-03/2024-03-12_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-03/2024-03-07_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-03/2024-03-04_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-03/2024-03-13_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-03/2024-03-11_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-03/2024-03-01_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-03/2024-03-22_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-03/2024-03-14_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-03/2024-03-21_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-03/2024-03-15_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-03/2024-03-26_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-03/2024-03-20_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-03/2024-03-06_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-06/2024-06-06_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-06/2024-06-05_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-06/2024-06-04_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-06/2024-06-03_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-06/2024-06-25_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-06/2024-06-24_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-06/2024-06-19_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-06/2024-06-20_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-06/2024-06-07_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-06/2024-06-12_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-06/2024-06-14_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-06/2024-06-13_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-06/2024-06-18_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-06/2024-06-21_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-06/2024-06-10_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-06/2024-06-28_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-06/2024-06-11_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-06/2024-06-27_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2024-06/2024-06-26_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2023-12/2023-12-01_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2023-12/2023-12-06_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2023-12/2023-12-15_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2023-12/2023-12-18_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2023-12/2023-12-20_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2023-12/2023-12-08_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2023-12/2023-12-05_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2023-12/2023-12-13_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2023-12/2023-12-12_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2023-12/2023-12-07_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2023-12/2023-12-21_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2023-12/2023-12-11_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2023-12/2023-12-22_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2023-12/2023-12-14_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2023-12/2023-12-27_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2023-12/2023-12-04_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2023-12/2023-12-28_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2023-12/2023-12-26_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2023-12/2023-12-29_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2023-12/2023-12-19_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2023-11/2023-11-16_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2023-11/2023-11-24_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2023-11/2023-11-15_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2023-11/2023-11-17_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2023-11/2023-11-03_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2023-11/2023-11-12_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2023-11/2023-11-29_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2023-11/2023-11-22_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2023-11/2023-11-23_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2023-11/2023-11-01_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2023-11/2023-11-07_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2023-11/2023-11-09_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2023-11/2023-11-20_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2023-11/2023-11-08_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2023-11/2023-11-10_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2023-11/2023-11-13_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2023-11/2023-11-28_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2023-11/2023-11-30_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2023-11/2023-11-02_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2023-11/2023-11-21_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2023-11/2023-11-06_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2023-10/2023-10-09_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2023-10/2023-10-04_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2023-10/2023-10-03_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2023-10/2023-10-05_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2023-10/2023-10-06_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2023-10/2023-10-31_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2023-10/2023-10-27_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2023-10/2023-10-13_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2023-10/2023-10-12_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2023-10/2023-10-18_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2023-10/2023-10-10_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2023-10/2023-10-17_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2023-10/2023-10-25_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2023-10/2023-10-11_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2023-10/2023-10-16_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2023-10/2023-10-30_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2023-10/2023-10-20_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2023-10/2023-10-26_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2023-10/2023-10-23_05-30-00.yaml\n",
            "Processing file: /content/drive/My Drive/Data-Driven Stock Analysis/2023-10/2023-10-19_05-30-00.yaml\n",
            "CSV created: /content/drive/My Drive/Assignment_CSV/symbol_CSV/SBIN.csv\n",
            "CSV created: /content/drive/My Drive/Assignment_CSV/symbol_CSV/BAJFINANCE.csv\n",
            "CSV created: /content/drive/My Drive/Assignment_CSV/symbol_CSV/TITAN.csv\n",
            "CSV created: /content/drive/My Drive/Assignment_CSV/symbol_CSV/ITC.csv\n",
            "CSV created: /content/drive/My Drive/Assignment_CSV/symbol_CSV/TCS.csv\n",
            "CSV created: /content/drive/My Drive/Assignment_CSV/symbol_CSV/LT.csv\n",
            "CSV created: /content/drive/My Drive/Assignment_CSV/symbol_CSV/TATACONSUM.csv\n",
            "CSV created: /content/drive/My Drive/Assignment_CSV/symbol_CSV/RELIANCE.csv\n",
            "CSV created: /content/drive/My Drive/Assignment_CSV/symbol_CSV/HCLTECH.csv\n",
            "CSV created: /content/drive/My Drive/Assignment_CSV/symbol_CSV/JSWSTEEL.csv\n",
            "CSV created: /content/drive/My Drive/Assignment_CSV/symbol_CSV/ULTRACEMCO.csv\n",
            "CSV created: /content/drive/My Drive/Assignment_CSV/symbol_CSV/POWERGRID.csv\n",
            "CSV created: /content/drive/My Drive/Assignment_CSV/symbol_CSV/INFY.csv\n",
            "CSV created: /content/drive/My Drive/Assignment_CSV/symbol_CSV/TRENT.csv\n",
            "CSV created: /content/drive/My Drive/Assignment_CSV/symbol_CSV/BHARTIARTL.csv\n",
            "CSV created: /content/drive/My Drive/Assignment_CSV/symbol_CSV/TATAMOTORS.csv\n",
            "CSV created: /content/drive/My Drive/Assignment_CSV/symbol_CSV/WIPRO.csv\n",
            "CSV created: /content/drive/My Drive/Assignment_CSV/symbol_CSV/TECHM.csv\n",
            "CSV created: /content/drive/My Drive/Assignment_CSV/symbol_CSV/NTPC.csv\n",
            "CSV created: /content/drive/My Drive/Assignment_CSV/symbol_CSV/HINDUNILVR.csv\n",
            "CSV created: /content/drive/My Drive/Assignment_CSV/symbol_CSV/APOLLOHOSP.csv\n",
            "CSV created: /content/drive/My Drive/Assignment_CSV/symbol_CSV/M&M.csv\n",
            "CSV created: /content/drive/My Drive/Assignment_CSV/symbol_CSV/GRASIM.csv\n",
            "CSV created: /content/drive/My Drive/Assignment_CSV/symbol_CSV/ICICIBANK.csv\n",
            "CSV created: /content/drive/My Drive/Assignment_CSV/symbol_CSV/ADANIENT.csv\n",
            "CSV created: /content/drive/My Drive/Assignment_CSV/symbol_CSV/ADANIPORTS.csv\n",
            "CSV created: /content/drive/My Drive/Assignment_CSV/symbol_CSV/BEL.csv\n",
            "CSV created: /content/drive/My Drive/Assignment_CSV/symbol_CSV/BAJAJFINSV.csv\n",
            "CSV created: /content/drive/My Drive/Assignment_CSV/symbol_CSV/EICHERMOT.csv\n",
            "CSV created: /content/drive/My Drive/Assignment_CSV/symbol_CSV/COALINDIA.csv\n",
            "CSV created: /content/drive/My Drive/Assignment_CSV/symbol_CSV/MARUTI.csv\n",
            "CSV created: /content/drive/My Drive/Assignment_CSV/symbol_CSV/INDUSINDBK.csv\n",
            "CSV created: /content/drive/My Drive/Assignment_CSV/symbol_CSV/ASIANPAINT.csv\n",
            "CSV created: /content/drive/My Drive/Assignment_CSV/symbol_CSV/TATASTEEL.csv\n",
            "CSV created: /content/drive/My Drive/Assignment_CSV/symbol_CSV/HDFCLIFE.csv\n",
            "CSV created: /content/drive/My Drive/Assignment_CSV/symbol_CSV/DRREDDY.csv\n",
            "CSV created: /content/drive/My Drive/Assignment_CSV/symbol_CSV/SUNPHARMA.csv\n",
            "CSV created: /content/drive/My Drive/Assignment_CSV/symbol_CSV/KOTAKBANK.csv\n",
            "CSV created: /content/drive/My Drive/Assignment_CSV/symbol_CSV/SHRIRAMFIN.csv\n",
            "CSV created: /content/drive/My Drive/Assignment_CSV/symbol_CSV/NESTLEIND.csv\n",
            "CSV created: /content/drive/My Drive/Assignment_CSV/symbol_CSV/ONGC.csv\n",
            "CSV created: /content/drive/My Drive/Assignment_CSV/symbol_CSV/CIPLA.csv\n",
            "CSV created: /content/drive/My Drive/Assignment_CSV/symbol_CSV/BPCL.csv\n",
            "CSV created: /content/drive/My Drive/Assignment_CSV/symbol_CSV/BRITANNIA.csv\n",
            "CSV created: /content/drive/My Drive/Assignment_CSV/symbol_CSV/SBILIFE.csv\n",
            "CSV created: /content/drive/My Drive/Assignment_CSV/symbol_CSV/HINDALCO.csv\n",
            "CSV created: /content/drive/My Drive/Assignment_CSV/symbol_CSV/HEROMOTOCO.csv\n",
            "CSV created: /content/drive/My Drive/Assignment_CSV/symbol_CSV/AXISBANK.csv\n",
            "CSV created: /content/drive/My Drive/Assignment_CSV/symbol_CSV/HDFCBANK.csv\n",
            "CSV created: /content/drive/My Drive/Assignment_CSV/symbol_CSV/BAJAJ-AUTO.csv\n",
            "📂 Scanning CSV files in: /content/drive/My Drive/Assignment_CSV/symbol_CSV\n",
            "📄 Loading file: /content/drive/My Drive/Assignment_CSV/symbol_CSV/SBIN.csv\n",
            "📄 Loading file: /content/drive/My Drive/Assignment_CSV/symbol_CSV/BAJFINANCE.csv\n",
            "📄 Loading file: /content/drive/My Drive/Assignment_CSV/symbol_CSV/TITAN.csv\n",
            "📄 Loading file: /content/drive/My Drive/Assignment_CSV/symbol_CSV/ITC.csv\n",
            "📄 Loading file: /content/drive/My Drive/Assignment_CSV/symbol_CSV/TCS.csv\n",
            "📄 Loading file: /content/drive/My Drive/Assignment_CSV/symbol_CSV/LT.csv\n",
            "📄 Loading file: /content/drive/My Drive/Assignment_CSV/symbol_CSV/TATACONSUM.csv\n",
            "📄 Loading file: /content/drive/My Drive/Assignment_CSV/symbol_CSV/RELIANCE.csv\n",
            "📄 Loading file: /content/drive/My Drive/Assignment_CSV/symbol_CSV/HCLTECH.csv\n",
            "📄 Loading file: /content/drive/My Drive/Assignment_CSV/symbol_CSV/JSWSTEEL.csv\n",
            "📄 Loading file: /content/drive/My Drive/Assignment_CSV/symbol_CSV/ULTRACEMCO.csv\n",
            "📄 Loading file: /content/drive/My Drive/Assignment_CSV/symbol_CSV/POWERGRID.csv\n",
            "📄 Loading file: /content/drive/My Drive/Assignment_CSV/symbol_CSV/INFY.csv\n",
            "📄 Loading file: /content/drive/My Drive/Assignment_CSV/symbol_CSV/TRENT.csv\n",
            "📄 Loading file: /content/drive/My Drive/Assignment_CSV/symbol_CSV/BHARTIARTL.csv\n",
            "📄 Loading file: /content/drive/My Drive/Assignment_CSV/symbol_CSV/TATAMOTORS.csv\n",
            "📄 Loading file: /content/drive/My Drive/Assignment_CSV/symbol_CSV/WIPRO.csv\n",
            "📄 Loading file: /content/drive/My Drive/Assignment_CSV/symbol_CSV/TECHM.csv\n",
            "📄 Loading file: /content/drive/My Drive/Assignment_CSV/symbol_CSV/NTPC.csv\n",
            "📄 Loading file: /content/drive/My Drive/Assignment_CSV/symbol_CSV/HINDUNILVR.csv\n",
            "📄 Loading file: /content/drive/My Drive/Assignment_CSV/symbol_CSV/APOLLOHOSP.csv\n",
            "📄 Loading file: /content/drive/My Drive/Assignment_CSV/symbol_CSV/M&M.csv\n",
            "📄 Loading file: /content/drive/My Drive/Assignment_CSV/symbol_CSV/GRASIM.csv\n",
            "📄 Loading file: /content/drive/My Drive/Assignment_CSV/symbol_CSV/ICICIBANK.csv\n",
            "📄 Loading file: /content/drive/My Drive/Assignment_CSV/symbol_CSV/ADANIENT.csv\n",
            "📄 Loading file: /content/drive/My Drive/Assignment_CSV/symbol_CSV/ADANIPORTS.csv\n",
            "📄 Loading file: /content/drive/My Drive/Assignment_CSV/symbol_CSV/BEL.csv\n",
            "📄 Loading file: /content/drive/My Drive/Assignment_CSV/symbol_CSV/BAJAJFINSV.csv\n",
            "📄 Loading file: /content/drive/My Drive/Assignment_CSV/symbol_CSV/EICHERMOT.csv\n",
            "📄 Loading file: /content/drive/My Drive/Assignment_CSV/symbol_CSV/COALINDIA.csv\n",
            "📄 Loading file: /content/drive/My Drive/Assignment_CSV/symbol_CSV/MARUTI.csv\n",
            "📄 Loading file: /content/drive/My Drive/Assignment_CSV/symbol_CSV/INDUSINDBK.csv\n",
            "📄 Loading file: /content/drive/My Drive/Assignment_CSV/symbol_CSV/ASIANPAINT.csv\n",
            "📄 Loading file: /content/drive/My Drive/Assignment_CSV/symbol_CSV/TATASTEEL.csv\n",
            "📄 Loading file: /content/drive/My Drive/Assignment_CSV/symbol_CSV/HDFCLIFE.csv\n",
            "📄 Loading file: /content/drive/My Drive/Assignment_CSV/symbol_CSV/DRREDDY.csv\n",
            "📄 Loading file: /content/drive/My Drive/Assignment_CSV/symbol_CSV/SUNPHARMA.csv\n",
            "📄 Loading file: /content/drive/My Drive/Assignment_CSV/symbol_CSV/KOTAKBANK.csv\n",
            "📄 Loading file: /content/drive/My Drive/Assignment_CSV/symbol_CSV/SHRIRAMFIN.csv\n",
            "📄 Loading file: /content/drive/My Drive/Assignment_CSV/symbol_CSV/NESTLEIND.csv\n",
            "📄 Loading file: /content/drive/My Drive/Assignment_CSV/symbol_CSV/ONGC.csv\n",
            "📄 Loading file: /content/drive/My Drive/Assignment_CSV/symbol_CSV/CIPLA.csv\n",
            "📄 Loading file: /content/drive/My Drive/Assignment_CSV/symbol_CSV/BPCL.csv\n",
            "📄 Loading file: /content/drive/My Drive/Assignment_CSV/symbol_CSV/BRITANNIA.csv\n",
            "📄 Loading file: /content/drive/My Drive/Assignment_CSV/symbol_CSV/SBILIFE.csv\n",
            "📄 Loading file: /content/drive/My Drive/Assignment_CSV/symbol_CSV/HINDALCO.csv\n",
            "📄 Loading file: /content/drive/My Drive/Assignment_CSV/symbol_CSV/HEROMOTOCO.csv\n",
            "📄 Loading file: /content/drive/My Drive/Assignment_CSV/symbol_CSV/AXISBANK.csv\n",
            "📄 Loading file: /content/drive/My Drive/Assignment_CSV/symbol_CSV/HDFCBANK.csv\n",
            "📄 Loading file: /content/drive/My Drive/Assignment_CSV/symbol_CSV/BAJAJ-AUTO.csv\n",
            "✅ Combined data saved to: /content/drive/My Drive/Assignment_CSV/combined_data.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stock_Data Imported into TiDB\n",
        "\n",
        "import pandas as pd\n",
        "import pymysql\n",
        "from sqlalchemy import create_engine\n",
        "\n",
        "# 🔹 Step 1: Mount Google Drive (Ensure CSV file is accessible)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 🔹 Step 2: Set Paths\n",
        "output_dir = \"/content/drive/My Drive/Assignment_CSV/symbol_CSV\"  # Folder with CSV files\n",
        "save_path = \"/content/drive/My Drive/Assignment_CSV/combined_data.csv\"  # Combined CSV path\n",
        "certificate_path = \"/content/drive/MyDrive/Assignment_CSV/isrgrootx1.pem\"  # SSL certificate path\n",
        "\n",
        "# 🔹 Step 4: Connect to TiDB\n",
        "tidb_host = \"gateway01.ap-southeast-1.prod.aws.tidbcloud.com\"\n",
        "tidb_port = 4000\n",
        "tidb_user = \"3nitUUKsZ9vMwCU.root\"\n",
        "tidb_password = \"rXPvkXcel0MlnGKI\"\n",
        "tidb_database = \"Project_2\"\n",
        "tidb_table = \"stock_data\"\n",
        "\n",
        "# Create connection with SSL\n",
        "connection = pymysql.connect(\n",
        "    host=tidb_host,\n",
        "    port=tidb_port,\n",
        "    user=tidb_user,\n",
        "    password=tidb_password,\n",
        "    ssl={'ca': certificate_path}  # Secure connection\n",
        ")\n",
        "\n",
        "cursor = connection.cursor()\n",
        "\n",
        "# 🔹 Step 5: Create Database (If Not Exists)\n",
        "cursor.execute(f\"CREATE DATABASE IF NOT EXISTS {tidb_database};\")\n",
        "print(f\"✅ Database '{tidb_database}' checked/created successfully!\")\n",
        "\n",
        "# Use the new database\n",
        "cursor.execute(f\"USE {tidb_database};\")\n",
        "\n",
        "# 🔹 Step 6: Create Table (If Not Exists)\n",
        "create_table_query = f\"\"\"\n",
        "CREATE TABLE IF NOT EXISTS {tidb_table} (\n",
        "    id INT AUTO_INCREMENT PRIMARY KEY,\n",
        "    ticker VARCHAR(512) NOT NULL,\n",
        "    date DATETIME,\n",
        "    close DOUBLE,\n",
        "    high DOUBLE,\n",
        "    low DOUBLE,\n",
        "    open DOUBLE,\n",
        "    volume BIGINT,\n",
        "    month VARCHAR(512)\n",
        ");\n",
        "\"\"\"\n",
        "cursor.execute(create_table_query)\n",
        "print(f\"✅ Table '{tidb_table}' checked/created successfully!\")\n",
        "\n",
        "# Commit the changes and close cursor\n",
        "connection.commit()\n",
        "cursor.close()\n",
        "connection.close()\n",
        "\n",
        "# 🔹 Step 7: Load Data from CSV\n",
        "df = pd.read_csv(save_path)\n",
        "\n",
        "# 🔹 Step 8: Insert Data into TiDB using SQLAlchemy\n",
        "if df is not None:\n",
        "    # Reconnect using SQLAlchemy for bulk insert\n",
        "    engine = create_engine(\n",
        "        f\"mysql+pymysql://{tidb_user}:{tidb_password}@{tidb_host}:{tidb_port}/{tidb_database}\",\n",
        "        connect_args={\"ssl\": {\"ca\": certificate_path}}\n",
        "    )\n",
        "\n",
        "    df.to_sql(tidb_table, engine, if_exists=\"append\", index=False)\n",
        "    print(f\"✅ Data successfully imported into '{tidb_table}' in TiDB!\")\n",
        "else:\n",
        "    print(\"❌ No data to import!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbnjH_myOD04",
        "outputId": "e9e77d09-6ea6-4760-b363-393f8a364c27"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "✅ Database 'Project_2' checked/created successfully!\n",
            "✅ Table 'stock_data' checked/created successfully!\n",
            "✅ Data successfully imported into 'stock_data' in TiDB!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Sector File Import in TIDB\n",
        "\n",
        "import pandas as pd\n",
        "from sqlalchemy import create_engine\n",
        "\n",
        "tidb_host = \"gateway01.ap-southeast-1.prod.aws.tidbcloud.com\"\n",
        "tidb_port = 4000\n",
        "tidb_user = \"3nitUUKsZ9vMwCU.root\"\n",
        "tidb_password = \"rXPvkXcel0MlnGKI\"\n",
        "tidb_database = \"Project_2\"\n",
        "tidb_table = \"sector_data\"\n",
        "certificate_path = \"/content/drive/MyDrive/Assignment_CSV/isrgrootx1.pem\"\n",
        "\n",
        "# Create SQLAlchemy engine with SSL\n",
        "engine = create_engine(\n",
        "    f\"mysql+pymysql://{tidb_user}:{tidb_password}@{tidb_host}:{tidb_port}/{tidb_database}\",\n",
        "    connect_args={\"ssl\": {\"ca\": certificate_path}}\n",
        ")\n",
        "\n",
        "# Specify the path to your CSV file\n",
        "csv_file_path = \"/content/drive/MyDrive/Assignment_CSV/sector/sector_data.csv\"\n",
        "\n",
        "try:\n",
        "    # Read the CSV file into a pandas DataFrame\n",
        "    df = pd.read_csv(csv_file_path)\n",
        "\n",
        "    # Upload the DataFrame to TiDB\n",
        "    df.to_sql(tidb_table, engine, if_exists='replace', index=False)\n",
        "\n",
        "    print(f\"Data from '{csv_file_path}' successfully uploaded to TiDB table '{tidb_table}'\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: CSV file not found at {csv_file_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BvnUfXhk9qm2",
        "outputId": "77c5a185-fdc0-4972-fa39-1b54f97a9a11"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data from '/content/drive/MyDrive/Assignment_CSV/sector/sector_data.csv' successfully uploaded to TiDB table 'sector_data'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Data Cleaning in Stock Data\n",
        "\n",
        "import pandas as pd\n",
        "from sqlalchemy import create_engine, text\n",
        "import pymysql\n",
        "\n",
        "# Database credentials\n",
        "tidb_host = \"gateway01.ap-southeast-1.prod.aws.tidbcloud.com\"\n",
        "tidb_port = 4000\n",
        "tidb_user = \"3nitUUKsZ9vMwCU.root\"\n",
        "tidb_password = \"rXPvkXcel0MlnGKI\"\n",
        "tidb_database = \"Project_2\"\n",
        "certificate_path = \"/content/drive/MyDrive/Assignment_CSV/isrgrootx1.pem\"\n",
        "tidb_table = \"stock_data\"\n",
        "\n",
        "# Create engine with SSL for secure connection\n",
        "engine = create_engine(\n",
        "    f\"mysql+pymysql://{tidb_user}:{tidb_password}@{tidb_host}:{tidb_port}/{tidb_database}\",\n",
        "    connect_args={\"ssl\": {\"ca\": certificate_path}}\n",
        ")\n",
        "\n",
        "try:\n",
        "    # Read data from TiDB into DataFrame\n",
        "    query = f\"SELECT * FROM {tidb_table}\"\n",
        "    df = pd.read_sql_query(query, engine)\n",
        "\n",
        "    # Clean the data\n",
        "    df.drop_duplicates(subset=['ticker', 'date'], inplace=True)\n",
        "    df.fillna(0, inplace=True)\n",
        "    df['volume'] = pd.to_numeric(df['volume'], errors='coerce').fillna(0).astype(int)\n",
        "\n",
        "    # Convert 'date' column to datetime\n",
        "    df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
        "    df['day_of_week'] = df['date'].dt.dayofweek  # Monday=0, Sunday=6\n",
        "\n",
        "    # Remove weekends (Saturday=5, Sunday=6)\n",
        "    df = df[~df['day_of_week'].isin([5, 6])]\n",
        "    df.drop('day_of_week', axis=1, inplace=True)\n",
        "\n",
        "    # Split 'date' into separate date and time columns\n",
        "    df['date_only'] = df['date'].dt.date\n",
        "    df['time_only'] = df['date'].dt.time\n",
        "    df.drop('date', axis=1, inplace=True)\n",
        "\n",
        "    # Upload cleaned data back to TiDB\n",
        "    df.to_sql(tidb_table, engine, if_exists='replace', index=False)\n",
        "    print(f\"Cleaned and updated data in TiDB table '{tidb_table}'\")\n",
        "\n",
        "    # Rename columns in TiDB\n",
        "    with engine.connect() as connection:\n",
        "        connection.execute(text(\"ALTER TABLE stock_data CHANGE COLUMN date_only date DATE;\"))\n",
        "        connection.execute(text(\"ALTER TABLE sector_data CHANGE COLUMN symbol ticker VARCHAR(512);\")\n",
        "    )\n",
        "    print(\"Columns renamed successfully\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBf1jLWI9qpv",
        "outputId": "27a2ddee-5709-48b9-9efa-fca6abe01e91"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned and updated data in TiDB table 'stock_data'\n",
            "Columns renamed successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Cleanning in Sector_data CSV\n",
        "\n",
        "from sqlalchemy import create_engine, text\n",
        "import pymysql\n",
        "\n",
        "# Database credentials (replace with your actual credentials)\n",
        "tidb_host = \"gateway01.ap-southeast-1.prod.aws.tidbcloud.com\"\n",
        "tidb_port = 4000\n",
        "tidb_user = \"3nitUUKsZ9vMwCU.root\"\n",
        "tidb_password = \"rXPvkXcel0MlnGKI\"\n",
        "tidb_database = \"Project_2\"\n",
        "certificate_path = \"/content/drive/MyDrive/Assignment_CSV/isrgrootx1.pem\"\n",
        "tidb_table = \"sector_data\"\n",
        "\n",
        "# Create engine with SSL\n",
        "engine = create_engine(\n",
        "    f\"mysql+pymysql://{tidb_user}:{tidb_password}@{tidb_host}:{tidb_port}/{tidb_database}\",\n",
        "    connect_args={\"ssl\": {\"ca\": certificate_path}},\n",
        "    echo=True\n",
        ")\n",
        "\n",
        "try:\n",
        "    with engine.begin() as connection:\n",
        "        # Update ticker column values\n",
        "        connection.execute(text(f\"UPDATE {tidb_table} SET ticker = REPLACE(ticker, 'ADANIGREEN', 'ADANIENT');\"))\n",
        "        connection.execute(text(f\"UPDATE {tidb_table} SET ticker = REPLACE(ticker, 'AIRTEL', 'BHARTIARTL');\"))\n",
        "        connection.execute(text(f\"UPDATE {tidb_table} SET ticker = REPLACE(ticker, 'TATACONSUMER', 'TATACONSUM');\"))\n",
        "        print(\"Ticker column updated successfully.\")\n",
        "\n",
        "        # Insert new row\n",
        "        connection.execute(text(\n",
        "            f\"INSERT INTO {tidb_table} (COMPANY, sector, ticker) VALUES ('Britannia Industries Ltd', 'FOOD & TOBACCO', 'BRITANNIA');\"\n",
        "        ))\n",
        "        print(\"New row added successfully.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "QoCL1cmkzmaW",
        "outputId": "6b7ede6d-39d0-4ee9-faf3-51fed527a8b5"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-02-25 01:26:41,813 INFO sqlalchemy.engine.Engine SELECT DATABASE()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:sqlalchemy.engine.Engine:SELECT DATABASE()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-02-25 01:26:41,814 INFO sqlalchemy.engine.Engine [raw sql] {}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:sqlalchemy.engine.Engine:[raw sql] {}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-02-25 01:26:41,926 INFO sqlalchemy.engine.Engine SELECT @@sql_mode\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:sqlalchemy.engine.Engine:SELECT @@sql_mode\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-02-25 01:26:41,927 INFO sqlalchemy.engine.Engine [raw sql] {}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:sqlalchemy.engine.Engine:[raw sql] {}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-02-25 01:26:41,981 INFO sqlalchemy.engine.Engine SELECT @@lower_case_table_names\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:sqlalchemy.engine.Engine:SELECT @@lower_case_table_names\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-02-25 01:26:41,983 INFO sqlalchemy.engine.Engine [raw sql] {}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:sqlalchemy.engine.Engine:[raw sql] {}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-02-25 01:26:42,091 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:sqlalchemy.engine.Engine:BEGIN (implicit)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-02-25 01:26:42,092 INFO sqlalchemy.engine.Engine UPDATE sector_data SET ticker = REPLACE(ticker, 'ADANIGREEN', 'ADANIENT');\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:sqlalchemy.engine.Engine:UPDATE sector_data SET ticker = REPLACE(ticker, 'ADANIGREEN', 'ADANIENT');\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-02-25 01:26:42,093 INFO sqlalchemy.engine.Engine [generated in 0.00109s] {}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:sqlalchemy.engine.Engine:[generated in 0.00109s] {}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-02-25 01:26:42,151 INFO sqlalchemy.engine.Engine UPDATE sector_data SET ticker = REPLACE(ticker, 'AIRTEL', 'BHARTIARTL');\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:sqlalchemy.engine.Engine:UPDATE sector_data SET ticker = REPLACE(ticker, 'AIRTEL', 'BHARTIARTL');\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-02-25 01:26:42,152 INFO sqlalchemy.engine.Engine [generated in 0.00135s] {}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:sqlalchemy.engine.Engine:[generated in 0.00135s] {}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-02-25 01:26:42,208 INFO sqlalchemy.engine.Engine UPDATE sector_data SET ticker = REPLACE(ticker, 'TATACONSUMER', 'TATACONSUM');\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:sqlalchemy.engine.Engine:UPDATE sector_data SET ticker = REPLACE(ticker, 'TATACONSUMER', 'TATACONSUM');\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-02-25 01:26:42,210 INFO sqlalchemy.engine.Engine [generated in 0.00166s] {}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:sqlalchemy.engine.Engine:[generated in 0.00166s] {}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ticker column updated successfully.\n",
            "2025-02-25 01:26:42,268 INFO sqlalchemy.engine.Engine INSERT INTO sector_data (COMPANY, sector, ticker) VALUES ('Britannia Industries Ltd', 'FOOD & TOBACCO', 'BRITANNIA');\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:sqlalchemy.engine.Engine:INSERT INTO sector_data (COMPANY, sector, ticker) VALUES ('Britannia Industries Ltd', 'FOOD & TOBACCO', 'BRITANNIA');\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-02-25 01:26:42,269 INFO sqlalchemy.engine.Engine [generated in 0.00127s] {}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:sqlalchemy.engine.Engine:[generated in 0.00127s] {}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New row added successfully.\n",
            "2025-02-25 01:26:42,324 INFO sqlalchemy.engine.Engine COMMIT\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:sqlalchemy.engine.Engine:COMMIT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "fur3nnmJVUT7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "912a4ff5-c976-43db-d10c-15bf43d3b6ea"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "from sqlalchemy import create_engine\n",
        "import pymysql\n",
        "\n",
        "# Database credentials (Consider using environment variables for security)\n",
        "tidb_host = \"gateway01.ap-southeast-1.prod.aws.tidbcloud.com\"\n",
        "tidb_port = 4000\n",
        "tidb_user = \"3nitUUKsZ9vMwCU.root\"\n",
        "tidb_password = \"rXPvkXcel0MlnGKI\"\n",
        "tidb_database = \"Project_2\"\n",
        "stock_table = \"stock_data\"\n",
        "sector_table = \"sector_data\"\n",
        "certificate_path = \"/content/drive/MyDrive/Assignment_CSV/isrgrootx1.pem\"\n",
        "\n",
        "\n",
        "# Create SQLAlchemy engine with SSL\n",
        "engine = create_engine(\n",
        "    f\"mysql+pymysql://{tidb_user}:{tidb_password}@{tidb_host}:{tidb_port}/{tidb_database}\",\n",
        "    connect_args={\"ssl\": {\"ca\": certificate_path}}\n",
        ")\n",
        "\n",
        "@st.cache_data\n",
        "def load_data():\n",
        "    \"\"\"Load stock and sector data from TiDB\"\"\"\n",
        "    stock_query = f\"SELECT * FROM {stock_table}\"\n",
        "    sector_query = f\"SELECT * FROM {sector_table}\"\n",
        "\n",
        "    stock_df = pd.read_sql_query(stock_query, engine)\n",
        "    sector_df = pd.read_sql_query(sector_query, engine)\n",
        "\n",
        "    # Ensure 'date' column is in datetime format\n",
        "    stock_df['date'] = pd.to_datetime(stock_df['date'])\n",
        "\n",
        "    # Remove duplicates from sector data\n",
        "    sector_df = sector_df.drop_duplicates(subset=['ticker'])\n",
        "\n",
        "    # Merge stock and sector data\n",
        "    merged_df = pd.merge(stock_df, sector_df, on='ticker', how='left')\n",
        "\n",
        "    return merged_df\n",
        "\n",
        "# Load Data\n",
        "df = load_data()\n",
        "\n",
        "def set_sidebar_color(bg_color=\"#00AEC1\", text_color=\"#FFFFFF\", selectbox_text_color=\"#000000\"):\n",
        "    st.markdown(\n",
        "        f\"\"\"\n",
        "        <style>\n",
        "        /* Sidebar background and general text styling */\n",
        "        [data-testid=\"stSidebar\"] {{\n",
        "            background-color: {bg_color} !important;\n",
        "        }}\n",
        "\n",
        "        /* General sidebar text - white and bold */\n",
        "        [data-testid=\"stSidebar\"] * {{\n",
        "            color: {text_color} !important;\n",
        "            font-weight: normal !important;\n",
        "        }}\n",
        "\n",
        "        /* Selectbox label - white and bold */\n",
        "        [data-testid=\"stSidebar\"] .stSelectbox label {{\n",
        "            color: {text_color} !important;\n",
        "            font-weight: normal !important;\n",
        "        }}\n",
        "\n",
        "        /* Selectbox dropdown and selected text - black */\n",
        "        [data-testid=\"stSidebar\"] .stSelectbox div[data-baseweb=\"select\"] * {{\n",
        "            color: {selectbox_text_color} !important;\n",
        "        }}\n",
        "        </style>\n",
        "        \"\"\",\n",
        "        unsafe_allow_html=True\n",
        "    )\n",
        "\n",
        "# Sidebar Navigation\n",
        "set_sidebar_color(bg_color=\"#00AEC1\", text_color=\"#FFFFFF\", selectbox_text_color=\"#000000\")   ##E15F99\n",
        "st.sidebar.markdown(f'<iframe src=\"https://lottie.host/embed/234ce960-07ce-4fa8-8375-35afadf4297b/or8NrvgTaM.lottie\" width=\"100\" height=\"100\" style=\"border:none;\"></iframe>', unsafe_allow_html=True)  # Use st.markdown to embed the Lottie animation as an iframe\n",
        "st.sidebar.title(\"📈 Stock Market Analysis\")\n",
        "st.sidebar.subheader(\"Select an Analysis\")\n",
        "\n",
        "\n",
        "menu = st.sidebar.radio(\"Navigation\", [\"🏠 Home\", \"📊 Analysis\"])\n",
        "\n",
        "# Home Page\n",
        "if menu == \"🏠 Home\":\n",
        "    st.title(\"🏠 Welcome to Stock Market Analysis Dashboard\")\n",
        "    st.write(\"\"\"\n",
        "    This dashboard provides interactive stock market insights,\n",
        "    including volatility analysis, cumulative returns, sector performance,\n",
        "    stock correlations, and monthly gainers & losers.\n",
        "\n",
        "    Use the **sidebar navigation** to explore different analyses.\n",
        "    \"\"\")\n",
        "    st.markdown(f'<div style=\"display: flex; justify-content: flex-end;\"> <iframe src=\"https://lottie.host/embed/1e25b53d-e914-4fbd-9650-5373dd6b8d7a/VSa0qQMh3S.lottie\" width=\"250\" height=\"250\" style=\"border:none;\"></iframe>', unsafe_allow_html=True)\n",
        "\n",
        "\n",
        "# Analysis Section (Only appears when \"Analysis\" is selected)\n",
        "elif menu == \"📊 Analysis\":\n",
        "    st.sidebar.subheader(\"Select an Analysis Type\")\n",
        "\n",
        "    # Radio buttons for different analysis options\n",
        "    option = st.sidebar.radio(\"\",\n",
        "                              [\"Volatility Analysis\", \"Cumulative Return\", \"Sector Performance\", \"Stock Correlation\", \"Monthly Gainers & Losers\"])\n",
        "\n",
        "    # Volatility Analysis\n",
        "    if option == \"Volatility Analysis\":\n",
        "        st.title(\"📊 Top 10 Most Volatile Stocks\")\n",
        "        st.write(\"\"\"\n",
        "        Volatility represents the degree of variation in a stock's price.\n",
        "        A higher volatility indicates more risk and potential reward.\n",
        "        \"\"\")\n",
        "\n",
        "        # Calculate daily return\n",
        "        df['daily_return'] = df.groupby('ticker')['close'].pct_change()\n",
        "\n",
        "        # Calculate volatility (standard deviation of daily returns)\n",
        "        volatility = df.groupby('ticker')['daily_return'].std()\n",
        "\n",
        "        # Convert volatility Series to DataFrame\n",
        "        top_10_volatile = volatility.nlargest(10).reset_index()\n",
        "        top_10_volatile.columns = ['ticker', 'volatility']  # Rename columns for clarity\n",
        "\n",
        "        # Create a bar chart with distinct colors\n",
        "        fig = px.bar(\n",
        "            data_frame=top_10_volatile,  # Ensure DataFrame is explicitly passed\n",
        "            x='ticker',\n",
        "            y='volatility',\n",
        "            labels={'ticker': 'Stock Ticker', 'volatility': 'Volatility'},\n",
        "            title='Top 10 Most Volatile Stocks',\n",
        "            color='ticker',  # Assign a unique color to each stock\n",
        "            color_discrete_sequence=px.colors.qualitative.Bold  # Use a diverse color palette\n",
        "        )\n",
        "\n",
        "        # Display the Plotly chart in Streamlit\n",
        "        st.plotly_chart(fig)\n",
        "\n",
        "    # Cumulative Return Analysis\n",
        "    elif option == \"Cumulative Return\":\n",
        "        st.title(\"📈 Cumulative Return of Top 5 Performing Stocks\")\n",
        "\n",
        "        st.write(\"\"\"\n",
        "        Cumulative return represents the total gain or loss of a stock over time,\n",
        "        assuming all gains are reinvested.\n",
        "        This analysis highlights the top-performing stocks.\n",
        "        \"\"\")\n",
        "\n",
        "        # Ensure 'date' column is in datetime format\n",
        "        df['date'] = pd.to_datetime(df['date'])\n",
        "\n",
        "        # Sort data by date\n",
        "        df = df.sort_values(by=['ticker', 'date'])\n",
        "\n",
        "        # Compute daily returns\n",
        "        df['daily_return'] = df.groupby('ticker')['close'].pct_change()\n",
        "        df = df.dropna(subset=['daily_return'])\n",
        "\n",
        "        # Compute cumulative return using .transform()\n",
        "        df['cumulative_return'] = df.groupby('ticker')['daily_return'].transform(lambda x: (1 + x).cumprod()) - 1\n",
        "\n",
        "        # Get top 5 stocks based on the latest cumulative return\n",
        "        top_5_stocks = df.groupby('ticker')['cumulative_return'].last().nlargest(5).index\n",
        "\n",
        "        # Filter for top 5 stocks\n",
        "        top_5_df = df[df['ticker'].isin(top_5_stocks)]\n",
        "\n",
        "        # Plot cumulative return for top 5 stocks\n",
        "        fig = px.line(top_5_df, x='date', y='cumulative_return', color='ticker',\n",
        "                      labels={'date': 'Date', 'cumulative_return': 'Cumulative Return'},\n",
        "                      title='📌 Cumulative Return for Top 5 Stocks')\n",
        "\n",
        "        st.plotly_chart(fig)\n",
        "\n",
        "\n",
        "    # Sector Performance\n",
        "    elif option == \"Sector Performance\":\n",
        "        st.title(\"📊 Average Yearly Return by Sector\")\n",
        "\n",
        "        st.write(\"\"\"\n",
        "        This analysis calculates the average yearly return of different sectors,\n",
        "        helping to identify the best and worst-performing industries.\n",
        "        \"\"\")\n",
        "\n",
        "        stock_query = f\"SELECT * FROM {stock_table}\"\n",
        "        stock_df = pd.read_sql_query(stock_query, engine)\n",
        "\n",
        "        # Read sector data\n",
        "        sector_query = f\"SELECT * FROM {sector_table}\"\n",
        "        sector_df = pd.read_sql_query(sector_query, engine)\n",
        "        # Ensure 'date' is in datetime format\n",
        "        stock_df['date'] = pd.to_datetime(stock_df['date'])\n",
        "\n",
        "        # Remove duplicates from sector data (if any)\n",
        "        sector_df = sector_df.drop_duplicates(subset=['ticker'])\n",
        "\n",
        "        # Map sector information to stock_df\n",
        "        stock_df['sector'] = stock_df['ticker'].map(sector_df.set_index('ticker')['sector'])\n",
        "\n",
        "        # Sort data by date in ascending order before aggregation\n",
        "        stock_df = stock_df.sort_values(by=['date', 'ticker'], ascending=True)\n",
        "\n",
        "        # Extract year\n",
        "        stock_df['year'] = stock_df['date'].dt.year\n",
        "\n",
        "\n",
        "        # Calculate yearly returns and include sector\n",
        "        yearly_returns = stock_df.groupby(['ticker', 'year', 'sector']).agg(\n",
        "            first_close=('close', 'first'),\n",
        "            last_close=('close', 'last')\n",
        "        ).reset_index()\n",
        "\n",
        "        # Compute yearly return percentage\n",
        "        yearly_returns['yearly_return'] = (yearly_returns['last_close'] / yearly_returns['first_close']) - 1\n",
        "\n",
        "        # Group by sector and calculate average yearly return\n",
        "        sector_performance = yearly_returns.groupby('sector')['yearly_return'].mean().reset_index()\n",
        "\n",
        "        # Rename column for clarity\n",
        "        sector_performance.columns = ['sector', 'avg_yearly_return']\n",
        "\n",
        "        # Create the Plotly bar chart\n",
        "        fig = px.bar(\n",
        "            data_frame=sector_performance,\n",
        "            x='sector',\n",
        "            y='avg_yearly_return',\n",
        "            labels={'sector': 'Sector', 'avg_yearly_return': 'Average Yearly Return'},\n",
        "            title=\"📌 Sector-wise Average Yearly Return\",\n",
        "            color='sector',  # Assign a unique color to each sector\n",
        "            color_discrete_sequence=px.colors.qualitative.Dark24  # Use a diverse color palette\n",
        "        )\n",
        "\n",
        "        # Display the Plotly chart in Streamlit\n",
        "        st.plotly_chart(fig)\n",
        "\n",
        "    # Stock Correlation\n",
        "    elif option == \"Stock Correlation\":\n",
        "        st.title(\"📊 Stock Price Correlation Heatmap\")\n",
        "\n",
        "        pivot_df = df.pivot(index='date', columns='ticker', values='close')\n",
        "        correlation_matrix = pivot_df.corr()\n",
        "\n",
        "        fig = px.imshow(correlation_matrix, labels=dict(x=\"Stock Ticker\", y=\"Stock Ticker\", color=\"Correlation\"),\n",
        "                        x=correlation_matrix.columns, y=correlation_matrix.columns, color_continuous_scale='RdBu',\n",
        "                        title=\"Stock Price Correlation Heatmap\")\n",
        "\n",
        "        fig.update_layout(\n",
        "            width=1000,  # Increase width\n",
        "            height=800,  # Increase height\n",
        "            xaxis_title=\"Stock Ticker\",\n",
        "            yaxis_title=\"Stock Ticker\",\n",
        "            title={'text': \"Stock Price Correlation Heatmap\", 'y': 0.95, 'x': 0.5, 'xanchor': 'center', 'yanchor': 'top'}\n",
        "        )\n",
        "        st.plotly_chart(fig)\n",
        "\n",
        "    # Monthly Gainers & Losers\n",
        "    elif option == \"Monthly Gainers & Losers\":\n",
        "        st.title(\"📊 Monthly Gainers & Losers\")\n",
        "\n",
        "        df['month'] = df['date'].dt.month\n",
        "        df['year'] = df['date'].dt.year\n",
        "\n",
        "        selected_year = st.sidebar.selectbox(\"📆 Select Year\", df['year'].unique())\n",
        "        selected_month = st.sidebar.selectbox(\"📆 Select Month\", df['month'].unique())\n",
        "\n",
        "        st.write(f\"\"\"\n",
        "        Below are the top 5 gaining and losing stocks for **{selected_year}-{selected_month:02d}**\n",
        "        based on their **monthly return** percentage.\n",
        "        \"\"\")\n",
        "\n",
        "        monthly_data = df[(df['year'] == selected_year) & (df['month'] == selected_month)]\n",
        "\n",
        "        if not monthly_data.empty:\n",
        "            monthly_data['monthly_return'] = monthly_data.groupby('ticker')['close'].pct_change()\n",
        "            monthly_data['monthly_return'] = monthly_data['monthly_return'].fillna(0)\n",
        "\n",
        "            last_monthly_returns = monthly_data.groupby('ticker')['monthly_return'].last()\n",
        "            top_5_gainers = last_monthly_returns.nlargest(5)\n",
        "            top_5_losers = last_monthly_returns.nsmallest(5)\n",
        "\n",
        "            col1, col2 = st.columns(2)\n",
        "\n",
        "            with col1:\n",
        "                fig_gainers = px.bar(x=top_5_gainers.index, y=top_5_gainers.values,\n",
        "                                    labels={'x': 'Stock Ticker', 'y': 'Monthly Return'},\n",
        "                                    title=f' 🟢 Top 5 Gainers - {selected_year}-{selected_month:02}',\n",
        "                                    color_discrete_sequence=['green']  # Setting bars to green\n",
        "                                    )\n",
        "                st.plotly_chart(fig_gainers)\n",
        "\n",
        "            with col2:\n",
        "                fig_losers = px.bar(x=top_5_losers.index, y=top_5_losers.values,\n",
        "                                    labels={'x': 'Stock Ticker', 'y': 'Monthly Return'},\n",
        "                                    title=f' 🔴 Top 5 Losers - {selected_year}-{selected_month:02}',\n",
        "                                    color_discrete_sequence=['red'])  # Setting bars to red\n",
        "                st.plotly_chart(fig_losers)\n",
        "\n",
        "        else:\n",
        "            st.warning(f\"⚠ No data found for {selected_year}-{selected_month:02}\")\n",
        "\n",
        "# Footer\n",
        "st.sidebar.markdown(\"---\")\n",
        "st.sidebar.info(\n",
        "    \"📊 **Stock Analysis Dashboard**  \\n\"\n",
        "    \"👩‍💻 Developed by **SATHISH**  \\n\"\n",
        "    \"📡 Data Source: TiDB  \\n\"\n",
        "    \"🔗 [GitHub Repository](https://github.com/SathishMalaVish/Data-Driven-Stock-Analysis)\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HkLMrLq1Xk9-",
        "outputId": "4faefd66-5a4a-4edd-c44c-3a07de47eda2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-02-25 01:18:42.690 No runtime found, using MemoryCacheStorageManager\n",
            "2025-02-25 01:18:42.696 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-25 01:18:42.696 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-25 01:18:42.698 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-25 01:18:42.699 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-25 01:18:42.700 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-25 01:18:42.701 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-25 01:18:42.701 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-25 01:18:42.702 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-25 01:18:42.703 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-25 01:18:42.704 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-25 01:18:42.704 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-25 01:18:42.706 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-25 01:18:42.706 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-25 01:18:42.707 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-25 01:18:42.710 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-25 01:18:42.711 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-25 01:18:42.711 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-25 01:18:42.712 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-25 01:18:42.713 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-25 01:18:42.714 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-25 01:18:42.715 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-25 01:18:42.715 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-25 01:18:42.716 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-25 01:18:42.717 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-25 01:18:42.717 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-25 01:18:42.718 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeltaGenerator(_root_container=1, _parent=DeltaGenerator())"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Set your ngrok auth token\n",
        "from pyngrok import ngrok\n",
        "ngrok.set_auth_token(\"2qZXTI70wbnYK9o4Ldi7bDiVMwX_6N2MUv8REzFnRsePg6zhh\")\n",
        "\n",
        "\n",
        "# Start ngrok tunnel\n",
        "public_url = ngrok.connect(8501)\n",
        "print(f\"Streamlit app is live at: {public_url}\")\n",
        "\n",
        "# Run the Streamlit app\n",
        "!streamlit run work.py &>/dev/null&"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkN3K-9dX3bW",
        "outputId": "c93901df-5158-46c2-db1d-7183d9a06154"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streamlit app is live at: NgrokTunnel: \"https://6ca0-35-236-184-48.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}